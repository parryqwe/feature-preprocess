{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值差補"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropna(axis,how,thresh,subset),drop(label,axis,inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputer(missing_values,strategy,axis),fillna(method,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.Series([1,np.nan,\"hello\",None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals2=np.array([1,np.nan,3,4])\n",
    "vals2.dtype\n",
    "1+np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nansum(vals2)\n",
    "np.nanmin(vals2)\n",
    "np.nanmax(vals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "2    hello\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "2    hello\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns',how='all')\n",
    "df.dropna(how=\"any\")\n",
    "df.dropna(how=\"all\")\n",
    "df.dropna(subset=[])\n",
    "df.dropna(axis='rows',thresh=3)       ##thresh指定空值至少多少個該列或欄才會刪除\n",
    "data.fillna(0)\n",
    "data.fillna(method='ffill')\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2\n",
       "0  1.0  NaN  2\n",
       "1  2.0  3.0  5\n",
       "2  NaN  4.0  6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([[1,np.nan,2],[2,3,5],[np.nan,4,6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2\n",
       "1  2.0  3.0  5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2\n",
       "0  2\n",
       "1  5\n",
       "2  6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2   3\n",
       "0  1.0  NaN  2 NaN\n",
       "1  2.0  3.0  5 NaN\n",
       "2  NaN  4.0  6 NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2\n",
       "0  1.0  NaN  2\n",
       "1  2.0  3.0  5\n",
       "2  NaN  4.0  6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis='columns',how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2   3\n",
       "1  2.0  3.0  5 NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=\"rows\",thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.Series([1,np.nan,2,None,3],index=list('abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1.0\n",
       "b    NaN\n",
       "c    2.0\n",
       "d    NaN\n",
       "e    3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1.0\n",
       "b    0.0\n",
       "c    2.0\n",
       "d    0.0\n",
       "e    3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1.0\n",
       "b    1.0\n",
       "c    2.0\n",
       "d    2.0\n",
       "e    3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1.0\n",
       "b    2.0\n",
       "c    2.0\n",
       "d    3.0\n",
       "e    3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2   3\n",
       "0  1.0  NaN  2 NaN\n",
       "1  2.0  3.0  5 NaN\n",
       "2  NaN  4.0  6 NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  1.0  1.0  2.0  2.0\n",
       "1  2.0  3.0  5.0  5.0\n",
       "2  NaN  4.0  6.0  6.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(method=\"ffill\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp=Imputer(strategy='mean')\n",
    "x2=imp.fit_transform(x)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()\n",
    "df.dropna(axis=1)\n",
    "df.dropna(how='all')\n",
    "df.dropna(thresh=4)\n",
    "df.dropna(subset=['C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2   3\n",
       "0  1.0  NaN  2 NaN\n",
       "1  2.0  3.0  5 NaN\n",
       "2  NaN  4.0  6 NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imr=Imputer(missing_values='NAN',strategy='mean',axis=0)\n",
    "imr=imr.fit(df)\n",
    "imputed_data=imr.transform(df.values)\n",
    "imputed_data\n",
    "\n",
    "imr=Imputer(missing_values='NAN',strategy='median',axis=0)\n",
    "imr=imr.fit(df)\n",
    "imputed_data=imr.transform(df.values)\n",
    "imputed_data\n",
    "\n",
    "\n",
    "imr=Imputer(missing_values='NAN',strategy='most_frequent',axis=0)\n",
    "imr=imr.fit(df)\n",
    "imputed_data=imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 DataFrame 空缺值的狀態\n",
    "def na_check(df_data):\n",
    "    data_na = (df_data.isnull().sum() / len(df_data)) * 100\n",
    "    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n",
    "    display(missing_data.head(10))\n",
    "na_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()/train.isnull().count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NA\n",
    "for i in train.select_dtypes(include=['object']).columns:\n",
    "    train[i][train[i].isnull()] = 'unknown'\n",
    "train= train.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 受雇日數為異常值的資料, 另外設一個欄位記錄, 並將異常的日數轉成空值 (np.nan)\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print (df.isnull().values.any())\n",
    "    if (df.isnull().values.any() == True):\n",
    "        columns_with_Nan = df.columns[df.isnull().any()].tolist()\n",
    "    print(columns_with_Nan)\n",
    "    for col in columns_with_Nan:\n",
    "        print(\"%s : %d\" % (col, df[col].isnull().sum()))\n",
    "    \n",
    "check_missing_values(train_merged)\n",
    "check_missing_values(test_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  移除出現在 training data 而沒有出現 testing data 中的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "# 調整欄位數, 移除出現在 training data 而沒有出現 testing data 中的欄位\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 離群直"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.describe()\n",
    "def remove_outlier(df_in, col_name):\n",
    "\n",
    "    #q1 = df_in[col_name].quantile(0.25)\n",
    "    #q3 = df_in[col_name].quantile(0.75)\n",
    "    #iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = 12\n",
    "    fence_high = 45\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "df_final_train=remove_outlier(train_merged,'bd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 GrLivArea 限制在 800 到 2500 以內, 調整離群值\n",
    "df['GrLivArea'] = df['GrLivArea'].clip(800, 2500)\n",
    "sns.regplot(x = df['GrLivArea'], y=train_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 GrLivArea 限制在 800 到 2500 以內, 捨棄離群值\n",
    "keep_indexs = (df['GrLivArea']> 800) & (df['GrLivArea']< 2500)\n",
    "df = df[keep_indexs]\n",
    "train_Y = train_Y[keep_indexs]\n",
    "sns.regplot(x = df['GrLivArea'], y=train_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去偏態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 LotArea 取 boxcox 後, 看散佈圖, 並計算分數\n",
    "from scipy import stats\n",
    "df_fixed = copy.deepcopy(df)\n",
    "df_fixed['LotArea'] = stats.boxcox(df_fixed['LotArea'], lmbda=0.15)\n",
    "sns.distplot(df_fixed['LotArea'][:train_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 將 Fare 取 log1p 後, 看散佈圖, 並計算分數\n",
    "df_fixed = copy.deepcopy(df)\n",
    "\"\"\"\n",
    "Your Code Here, transform df_fixed['Fare'] with log1p\n",
    "\"\"\"\n",
    "df_fixed['Fare'] = np.log1p(df_fixed['Fare'])\n",
    "sns.distplot(df_fixed['Fare'][:train_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連續變數類別化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Fare'] = df_data['Fare'].fillna(df_data['Fare'].median())\n",
    "\n",
    "# Making Bins\n",
    "df_data['FareBin_4'] = pd.qcut(df_data['Fare'], 4)\n",
    "df_data['FareBin_5'] = pd.qcut(df_data['Fare'], 5)\n",
    "df_data['FareBin_6'] = pd.qcut(df_data['Fare'], 6)\n",
    "\n",
    "label = LabelEncoder()\n",
    "df_data['FareBin_Code_4'] = label.fit_transform(df_data['FareBin_4'])\n",
    "df_data['FareBin_Code_5'] = label.fit_transform(df_data['FareBin_5'])\n",
    "df_data['FareBin_Code_6'] = label.fit_transform(df_data['FareBin_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類別變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to categoty\n",
    "train['registration_init_time'] = train['registration_init_time'].astype('category')\n",
    "train['expiration_date'] = train['expiration_date'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object data to category\n",
    "for col in train.select_dtypes(include=['object']).columns:\n",
    "    train[col] = train[col].astype('category')\n",
    "    \n",
    "# Encoding categorical features\n",
    "for col in train.select_dtypes(include=['category']).columns:\n",
    "    train[col] = train[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Embarked', 'Title', 'Pclass'] #'Age', 'Pclass'\n",
    "for var in categorical:\n",
    "    test_df = pd.concat([test_df, pd.get_dummies(test_df[var], prefix=var)], axis=1)\n",
    "    del test_df[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[{'price':850000,'rooms':4,'neighborhood':'Queen Anne'},{'price':700000,'rooms':3,'neighborhood':'Fremont'},{'price':650000,'rooms':3,'neighborhood':'Wallingford'},{'price':600000,'rooms':2,'neighborhood':'Fremont'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      1,      0, 850000,      4],\n",
       "       [     1,      0,      0, 700000,      3],\n",
       "       [     0,      0,      1, 650000,      3],\n",
       "       [     1,      0,      0, 600000,      2]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec=DictVectorizer(sparse=False,dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighborhood=Fremont',\n",
       " 'neighborhood=Queen Anne',\n",
       " 'neighborhood=Wallingford',\n",
       " 'price',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x5 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec=DictVectorizer(sparse=True,dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mapping={'XL':3,'L':2,'M':1}\n",
    "df['size']=df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class_mapping={label:idx for idx,label in enumerate(np.unique(df['classlabel']))}\n",
    "df['classlabel']=df['classlabel'].map(class_mapping)\n",
    "df\n",
    "\n",
    "inv_class_mapping={v:k for k,v in class_mapping.items()}\n",
    "df['classlabel']=df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le=LabelEncoder()\n",
    "y=class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[['color','size','price']].values\n",
    "color_le=LabelEncoder()\n",
    "x[:,0]=color_le.fit_transform(x[:,0])\n",
    "x\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe=OneHotEncoder(categorical_features=[0])\n",
    "ohe.fit_transform(x).toarray()\n",
    "\n",
    "pd.get_dummies(df[['price','color','size']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵工程-簡化版 : 全部空值先補-1, 所有類別欄位先做 LabelEncoder, 然後再與數字欄位做 MinMaxScaler\n",
    "# 這邊使用 LabelEncoder 只是先將類別欄位用統一方式轉成數值以便輸入模型, 當然部分欄位做 One-Hot可能會更好, 只是先使用最簡單版本作為範例\n",
    "LEncoder = LabelEncoder()\n",
    "# 除上述之外, 還要把標籤編碼與數值欄位一起做最大最小化, 這麼做雖然有些暴力, 卻可以最簡單的平衡特徵間影響力\n",
    "MMEncoder = MinMaxScaler()\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == 'object': # 如果是文字型 / 類別型欄位, 就先補缺 'None' 後, 再做標籤編碼\n",
    "        df[c] = df[c].fillna('None')\n",
    "        df[c] = LEncoder.fit_transform(df[c]) \n",
    "    else: # 其他狀況(本例其他都是數值), 就補缺 -1\n",
    "        df[c] = df[c].fillna(-1)\n",
    "    # 最後, 將標籤編碼與數值欄位一起最大最小化, 因為需要是一維陣列, 所以這邊切出來後用 reshape 降維\n",
    "    df[c] = MMEncoder.fit_transform(df[c].values.reshape(-1, 1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確定只有 int64, float64, object 三種類型後對欄位名稱執行迴圈, 分別將欄位名稱存於三個 list 中\n",
    "int_features = []\n",
    "float_features = []\n",
    "object_features = []\n",
    "# .dtypes(欄位類型), .columns(欄位名稱) 是 DataFrame 提供的兩個方法, 這裡順便展示一下 for 與 zip 搭配的用法\n",
    "for dtype, feature in zip(df.dtypes, df.columns):\n",
    "    if dtype == 'float64':\n",
    "        float_features.append(feature)\n",
    "    elif dtype == 'int64':\n",
    "        int_features.append(feature)\n",
    "    else:\n",
    "        object_features.append(feature)\n",
    "# 這邊採用的寫法稱為 f-string, 是 Python 3.6.2 以後版本才出現的\n",
    "# 如果無法執行, 則需要更新到這個版本之後, 或自行將程式改寫為 str.format 形式\n",
    "# 改寫方式可以參考 https://blog.louie.lu/2017/08/08/outdate-python-string-format-and-fstring/\n",
    "print(f'{len(int_features)} Integer Features : {int_features}\\n')\n",
    "print(f'{len(float_features)} Float Features : {float_features}\\n')\n",
    "print(f'{len(object_features)} Object Features : {object_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上 'Cabin' 欄位的計數編碼\n",
    "count_df=df.groupby(['Cabin'])['Name'].agg({'Cabin_Count':'size'}).reset_index()\n",
    "df = pd.merge(df, count_df, on=['Cabin'], how='left')\n",
    "count_df.sort_values(by=['Cabin_Count'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Ticket'特徵雜湊 + 邏輯斯迴歸\n",
    "df_temp = pd.DataFrame()\n",
    "for c in object_features:\n",
    "    df_temp[c] = LabelEncoder().fit_transform(df[c])\n",
    "# 這邊的雜湊編碼, 是直接將 'Ticket' 的名稱放入雜湊函數的輸出數值, 為了要確定是緊密(dense)特徵, 因此除以10後看餘數\n",
    "# 這邊的 10 是隨機選擇, 不一定要用 10, 同學可以自由選擇購小的數字試看看. 基本上效果都不會太好\n",
    "df_temp['Ticket_Hash'] = df['Ticket'].map(lambda x:hash(x) % 10)\n",
    "train_X = df_temp[:train_num]\n",
    "estimator = LogisticRegression()\n",
    "print(cross_val_score(estimator, train_X, train_Y, cv=5).mean())\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Title 的 特徵工程 : 將各種頭銜按照類型分類, 最後取 One Hot\n",
    "df_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df[\"Name\"]]\n",
    "df[\"Title\"] = pd.Series(df_title)\n",
    "df[\"Title\"] = df[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "df[\"Title\"] = df[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "df[\"Title\"] = df[\"Title\"].astype(int)\n",
    "df = pd.get_dummies(df, columns = [\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked, Pclass 取 One Hot\n",
    "df = pd.get_dummies(df, columns = [\"Embarked\"], prefix=\"Em\")\n",
    "df[\"Pclass\"] = df[\"Pclass\"].astype(\"category\")\n",
    "df = pd.get_dummies(df, columns = [\"Pclass\"], prefix=\"Pc\")\n",
    "\n",
    "# 捨棄 Name 欄位\n",
    "df.drop(labels = [\"Name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時間特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時間轉換方式 : 使用 datetime.strptime 解析(parse)時間字串 / 使用 datetime.strftime 匯出時間格式(format)\n",
    "# 參考官網 https://docs.python.org/3/library/datetime.html\n",
    "df['pickup_datetime'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S UTC'))\n",
    "df['pickup_year'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%Y')).astype('int64')\n",
    "df['pickup_month'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%m')).astype('int64')\n",
    "df['pickup_day'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%d')).astype('int64')\n",
    "df['pickup_hour'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%H')).astype('int64')\n",
    "df['pickup_minute'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%M')).astype('int64')\n",
    "df['pickup_second'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%S')).astype('int64')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上\"日週期\"特徵 (參考講義\"週期循環特徵\")\n",
    "import math\n",
    "df['day_cycle'] = df['pickup_hour']/12 + df['pickup_minute']/720 + df['pickup_second']/43200\n",
    "df['day_cycle'] = df['day_cycle'].map(lambda x:math.sin(x*math.pi))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入星期幾與第幾周兩個特徵\n",
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "df['pickup_weekday'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%w')).astype('int64')\n",
    "df['pickup_week'] = df['pickup_datetime'].apply(lambda x: datetime.datetime.strftime(x, '%U')).astype('int64')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上\"年週期\"與\"周週期\"特徵\n",
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "import math\n",
    "df['year_cycle'] = df['pickup_month']/6 + df['pickup_day']/180 \n",
    "df['year_cycle'] = df['year_cycle'].map(lambda x:math.cos(x*math.pi))\n",
    "df['week_cycle'] = df['pickup_weekday']/3.5 + df['pickup_hour']/84 \n",
    "df['week_cycle'] = df['week_cycle'].map(lambda x:math.sin(x*math.pi))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=['problem of evil','evil queen','horizon problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec=CountVectorizer()\n",
    "x=vec.fit_transform(sample)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evil  horizon  of  problem  queen\n",
       "0     1        0   1        1      0\n",
       "1     1        0   0        0      1\n",
       "2     0        1   0        1      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x.toarray(),columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "corpus=['Time flies flies like an arrow.','Fruit flies like a banana.']\n",
    "one_hot_vectorizer=CountVectorizer(binary=True)\n",
    "one_hot=one_hot_vectorizer.fit_transform(corpus).toarray()\n",
    "sns.heatmap(one_hot,annot=True,cbar=False,xticklabels=vocab,yticklabels=['Sentence2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       evil   horizon        of   problem     queen\n",
       "0  0.517856  0.000000  0.680919  0.517856  0.000000\n",
       "1  0.605349  0.000000  0.000000  0.000000  0.795961\n",
       "2  0.000000  0.795961  0.000000  0.605349  0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec=TfidfVectorizer()\n",
    "x=vec.fit_transform(sample)\n",
    "pd.DataFrame(x.toarray(),columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "tfidf=tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "sns.heatmap(tfidf,annot=True,cbar=False,xticklabels=vocab,yticklabels=['Sentence1','Sentence2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連續變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeature\n",
    "poly=PolynomialFeatures(degree=3,include_bias=False)\n",
    "x2=poly.fit_transform(x)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms=MinMaxScaler()\n",
    "x_train_norm=mms.fit_transform(x_train)\n",
    "x_test_norm=mms.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc=StandardScaler()\n",
    "x_train_std=stdsc.fit_transform(x_train)\n",
    "x_test_std=stdsc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "Normalizer().fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log1p\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "FunctionTransformer(log1p).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 數值與數值組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\"\"\"\n",
    "Your Code Here, set new character at df['distance_real']\n",
    "\"\"\"\n",
    "latitude_diff2 = math.cos(df['dropoff_latitude'][0]) - math.cos(df['pickup_latitude'][0])\n",
    "df['distance_real'] = (df['longitude_diff']**2 + latitude_diff2**2)**0.5\n",
    "# 觀察結果 \n",
    "train_X = scaler.fit_transform(df)\n",
    "print(f'Linear Reg Score : {cross_val_score(Linear, train_X, train_Y, cv=5).mean()}')\n",
    "print(f'Gradient Boosting Reg Score : {cross_val_score(GDBT, train_X, train_Y, cv=5).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作四特徵 : 加, 乘, 互除(分母加1避免除0) 看效果 (Note: 數值原本已經最大最小化介於 [0,1] 區間, 這四種新特徵也會落在 [0,1] 區間)\n",
    "df['Add_char'] = (df['GrLivArea'] + df['OverallQual']) / 2\n",
    "df['Multi_char'] = df['GrLivArea'] * df['OverallQual']\n",
    "df['GO_div1p'] = df['GrLivArea'] / (df['OverallQual']+1) * 2\n",
    "df['OG_div1p'] = df['OverallQual'] / (df['GrLivArea']+1) * 2\n",
    "train_X = MMEncoder.fit_transform(df)\n",
    "cross_val_score(estimator, train_X, train_Y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  群聚編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生活總面積(GrLivArea)對販售條件(SaleCondition)做群聚編碼\n",
    "# 寫法類似均值編碼, 只是對另一個特徵, 而非目標值\n",
    "df['SaleCondition'] = df['SaleCondition'].fillna('None')\n",
    "mean_df = df.groupby(['SaleCondition'])['GrLivArea'].mean().reset_index()\n",
    "mode_df = df.groupby(['SaleCondition'])['GrLivArea'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "median_df = df.groupby(['SaleCondition'])['GrLivArea'].median().reset_index()\n",
    "max_df = df.groupby(['SaleCondition'])['GrLivArea'].max().reset_index()\n",
    "temp = pd.merge(mean_df, mode_df, how='left', on=['SaleCondition'])\n",
    "temp = pd.merge(temp, median_df, how='left', on=['SaleCondition'])\n",
    "temp = pd.merge(temp, max_df, how='left', on=['SaleCondition'])\n",
    "temp.columns = ['SaleCondition', 'Area_Sale_Mean', 'Area_Sale_Mode', 'Area_Sale_Median', 'Area_Sale_Max']\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分訓練集與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 篩選相關係數大於 0.1 或小於 -0.1 的特徵\n",
    "high_list = list(corr[(corr['SalePrice']>0.1) | (corr['SalePrice']<-0.1)].index)\n",
    "high_list.pop(-1)\n",
    "print(high_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "L1_Reg = Lasso(alpha=0.001)\n",
    "train_X = MMEncoder.fit_transform(df)\n",
    "L1_Reg.fit(train_X, train_Y)\n",
    "L1_Reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_mask = list((L1_Reg.coef_>0) | (L1_Reg.coef_<0))\n",
    "df.columns[L1_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import compress\n",
    "L1_mask = list((L1_Reg.coef_>0) | (L1_Reg.coef_<0))\n",
    "L1_list = list(compress(list(df), list(L1_mask)))\n",
    "L1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "VarianceThreshold(threshold=3).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "SelectKBest(lambda X,Y:array(map(lambda x:pearsonr(x,Y),x.T)).T,k=2).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "SelectKBest(chi2,k=2).fit_transform(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from minepy import MINE\n",
    "def mic(x,y):\n",
    "    m=MINE()\n",
    "    m.compute_score(x,y)\n",
    "    return (m.mic(),0.5)\n",
    "SelectKBest(lambda X,Y:array(map(lambda x:mic(x,Y),x.T)).T,k=2).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 循序特徵選擇演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBS():\n",
    "    def __init__(self,estimator,k_features,scoring=accuracy_score,test_size=0.25,random_state=1):\n",
    "        self.scoring=scoring\n",
    "        self.estimator=clone(estimator)\n",
    "        self.k_features=k_features\n",
    "        self.test_size=test_size\n",
    "        self.random_state=random_state\n",
    "    def fit(self,x,y):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=self.test_size,random_state=self.random_state)\n",
    "        dim=x_train.shape[1]\n",
    "        self.indices_=tuple(range(dim))\n",
    "        self.subsets_=[self.indices_]\n",
    "        score=self._calc_score(x_train,y_train,x_test,y_test,self.indices_)\n",
    "        self.scores_=[score]\n",
    "        while dim>self.k_features:\n",
    "            scores=[]\n",
    "            subsets=[]\n",
    "            \n",
    "            for p in combinations(self.indices_,r=dim-1):\n",
    "                score=self._calc_score(x_train,y_train,x_test,y_test,p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                \n",
    "            best=np.argmax(scores)\n",
    "            self.indices_=subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim-=1\n",
    "            \n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_=self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "    def transform(self,x):\n",
    "        return x[:,self.indices_]\n",
    "    def _calc_score(self,x_train,y_train,x_test,y_test,indices):\n",
    "        self.estimator.fit(x_train[:,indices],y_train)\n",
    "        y_pred=self.estimator.predict(x_test[:,indices])\n",
    "        score=self.scoring(y_test,y_pred)\n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot import plt\n",
    "knn=KNeighborsClassifier(n_neighbors=2)\n",
    "sbs=SBS(knn,k_features=1)\n",
    "sbs.fit(x_train_std,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_feat=[len(k) for k in sbs.subsets_]\n",
    "plt.plot(k_feat,sbs.scores_,marker='o')\n",
    "plt.ylim([0.7,1.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k5=list(sbs.subset_[8])\n",
    "print(df_wine.columns[1:][k5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "RFE(estimator=LogisticRegression(),n_features_to_selection=2).fit_transform(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 樹模型特徵選擇法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "SelectFromModel(GradientBoostingClassifier().fit_transform(iris.data,iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基於懲罰項的特徵選擇法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#带L1惩罚项的逻辑回归作为基模型的特征选择\n",
    "SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LR(LogisticRegression):\n",
    "    def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,\n",
    "                 fit_intercept=True, intercept_scaling=1, class_weight=None,\n",
    "                 random_state=None, solver='liblinear', max_iter=100,\n",
    "                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):\n",
    "\n",
    "        #权值相近的阈值\n",
    "        self.threshold = threshold\n",
    "        LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,\n",
    "                 fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,\n",
    "                 random_state=random_state, solver=solver, max_iter=max_iter,\n",
    "                 multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        #使用同样的参数创建L2逻辑回归\n",
    "        self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        #训练L1逻辑回归\n",
    "        super(LR, self).fit(X, y, sample_weight=sample_weight)\n",
    "        self.coef_old_ = self.coef_.copy()\n",
    "        #训练L2逻辑回归\n",
    "        self.l2.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        cntOfRow, cntOfCol = self.coef_.shape\n",
    "        #权值系数矩阵的行数对应目标值的种类数目\n",
    "        for i in range(cntOfRow):\n",
    "            for j in range(cntOfCol):\n",
    "                coef = self.coef_[i][j]\n",
    "                #L1逻辑回归的权值系数不为0\n",
    "                if coef != 0:\n",
    "                    idx = [j]\n",
    "                    #对应在L2逻辑回归中的权值系数\n",
    "                    coef1 = self.l2.coef_[i][j]\n",
    "                    for k in range(cntOfCol):\n",
    "                        coef2 = self.l2.coef_[i][k]\n",
    "                        #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0\n",
    "                        if abs(coef1-coef2) < self.threshold and j != k and self.coef_[i][k] == 0:\n",
    "                            idx.append(k)\n",
    "                    mean = coef / len(idx)\n",
    "                    self.coef_[i][idx] = mean\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    " \n",
    "#带L1和L2惩罚项的逻辑回归作为基模型的特征选择\n",
    "#参数threshold为权值系数之差的阈值\n",
    "SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隨機森林評估特徵重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "feat_labels=df_wine.columns[1:]\n",
    "forest=RandomForestClassifier(n_estimators=10000,random_state=0,n_jobs=-1)\n",
    "forest.fit(x_train,y_train)\n",
    "importances=forest.feature_importances_\n",
    "indices=np.argsort(importances)[::-1]\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%2d %-*s %f\" % (f+1,30,feat_labels[f],importances[indices[f]]))\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(x_train.shape[1]),importances[indices],color='lightblue',align='center')\n",
    "plt.xtick(range(x_train.shape[1]),feat_labels,rotation=90)\n",
    "plt.xlim([-1,x_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_selected=forest.transform(x_train,threshold=0.15)\n",
    "x_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 隨機森林擬合後, 將結果依照重要性由高到低排序\n",
    "estimator = RandomForestRegressor()\n",
    "estimator.fit(df.values, train_Y)\n",
    "# estimator.feature_importances_ 就是模型的特徵重要性, 這邊先與欄位名稱結合起來, 才能看到重要性與欄位名稱的對照表\n",
    "feats = pd.Series(data=estimator.feature_importances_, index=df.columns)\n",
    "feats = feats.sort_values(ascending=False)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高重要性特徵 + 隨機森林 (39大約是79的一半)\n",
    "high_feature = list(feats[:39].index)\n",
    "train_X = MMEncoder.fit_transform(df[high_feature])\n",
    "cross_val_score(estimator, train_X, train_Y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 觀察重要特徵與目標的分布\n",
    "# 第一名 : OverallQual              \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.regplot(x=df['OverallQual'], y=train_Y, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方差選擇法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_threshold_selector(data, threshold=.8 * (1 - .8)):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=variance_threshold_selector(data, threshold=.8 * (1 - .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 280)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE', 'EDUCATION_CD', 'L1YR_A_ISSUE_CNT', 'L1YR_B_ISSUE_CNT',\n",
       "       'CHANNEL_A_POL_CNT', 'CHANNEL_B_POL_CNT', 'OCCUPATION_CLASS_CD',\n",
       "       'APC_CNT', 'INSD_CNT', 'APC_1ST_AGE',\n",
       "       ...\n",
       "       'C_IND_Y', 'C_IND_unk', 'L1YR_LAPSE_IND_N', 'L1YR_LAPSE_IND_Y',\n",
       "       'L1YR_PAYMENT_REMINDER_IND_N', 'L1YR_PAYMENT_REMINDER_IND_Y',\n",
       "       'LAST_B_CONTACT_DT_N', 'LAST_B_CONTACT_DT_Y', 'LAST_C_DT_N',\n",
       "       'LAST_C_DT_Y'],\n",
       "      dtype='object', length=280)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=sel.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 134)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 71)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=new_data[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=new_data[100000:250000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve,accuracy_score,confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(X_train, X_test, y_train):\n",
    "    model = AdaBoostClassifier(n_estimators=500, random_state=42)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_auc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "i = 0\n",
    "for train_index, test_index in skf.split(train_data,train_y):\n",
    "    i +=1 \n",
    "    X_train, X_test = train_data[train_index], train_data[test_index]\n",
    "    y_train, y_test = train_y.loc[train_index], train_y.loc[test_index]\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n",
    "    y_smote = adaboost(X_train_sm, X_test, y_train_sm)\n",
    "    y_pred=[y_smote[i][1] for i in range(len(y_smote))]\n",
    "    score_auc.append(roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7183622448979591,\n",
       " 0.8148671556122449,\n",
       " 0.7171554209183673,\n",
       " 0.7752987244897959,\n",
       " 0.7803628826530612]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包裹式選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a2279\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n",
    "                           n_redundant=2, n_repeated=0, n_classes=8,\n",
    "                           n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(5),\n",
    "              scoring='roc_auc')\n",
    "rfecv.fit(train_data,train_y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 葉編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df.values\n",
    "# 因為訓練邏輯斯迴歸時也要資料, 因此將訓練及切成三部分 train / val / test, 採用 test 驗證而非 k-fold 交叉驗證\n",
    "# train 用來訓練梯度提升樹, val 用來訓練邏輯斯迴歸, test 驗證效果\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=0.5)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度提升樹調整參數並擬合後, 再將葉編碼 (*.apply) 結果做獨熱 / 邏輯斯迴歸\n",
    "# 調整參數的方式採用 RandomSearchCV 或 GridSearchCV, 以後的進度會再教給大家, 本次先直接使用調參結果\n",
    "gdbt = GradientBoostingClassifier(subsample=0.93, n_estimators=320, min_samples_split=0.1, min_samples_leaf=0.3, \n",
    "                                  max_features=4, max_depth=4, learning_rate=0.16)\n",
    "onehot = OneHotEncoder()\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "gdbt.fit(train_X, train_Y)\n",
    "onehot.fit(gdbt.apply(train_X)[:, :, 0])\n",
    "lr.fit(onehot.transform(gdbt.apply(val_X)[:, :, 0]), val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將梯度提升樹+葉編碼+邏輯斯迴歸結果輸出\n",
    "pred_gdbt_lr = lr.predict_proba(onehot.transform(gdbt.apply(test_X)[:, :, 0]))[:, 1]\n",
    "fpr_gdbt_lr, tpr_gdbt_lr, _ = roc_curve(test_Y, pred_gdbt_lr)\n",
    "# 將梯度提升樹結果輸出\n",
    "pred_gdbt = gdbt.predict_proba(test_X)[:, 1]\n",
    "fpr_gdbt, tpr_gdbt, _ = roc_curve(test_Y, pred_gdbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 將結果繪圖\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_gdbt, tpr_gdbt, label='GDBT')\n",
    "plt.plot(fpr_gdbt_lr, tpr_gdbt_lr, label='GDBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機森林擬合後, 再將葉編碼 (*.apply) 結果做獨熱 / 邏輯斯迴歸\n",
    "rf = RandomForestClassifier(n_estimators=20, min_samples_split=10, min_samples_leaf=5, \n",
    "                            max_features=4, max_depth=3, bootstrap=True)\n",
    "onehot = OneHotEncoder()\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "\"\"\"\n",
    "Your Code Here (Hint : 隨機森林的葉編碼(.apply)不需要加上[:, :, 0], 直接用rf.apply()調用即可, 本作業其餘寫法相同)\n",
    "\"\"\"\n",
    "rf.fit(train_X, train_Y)\n",
    "onehot.fit(rf.apply(train_X))\n",
    "lr.fit(onehot.transform(rf.apply(val_X)), val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將隨機森林+葉編碼+邏輯斯迴歸結果輸出\n",
    "\"\"\"\n",
    "Your Code Here \n",
    "\"\"\"\n",
    "pred_rf_lr = lr.predict_proba(onehot.transform(rf.apply(test_X)))[:, 1]\n",
    "fpr_rf_lr, tpr_rf_lr, _ = roc_curve(test_Y, pred_rf_lr)\n",
    "# 將隨機森林結果輸出\n",
    "\"\"\"\n",
    "Your Code Here \n",
    "\"\"\"\n",
    "pred_rf = rf.predict_proba(test_X)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(test_Y, pred_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
